1. Explain what is Thrift.

* A software framework  used for implementing Remote Procedure Call (RPC) in services, with cross-language support is known as Apache Thrift.*
*Thrift combines a software stack with a code generation engine to build cross-platform services which can connect applications written using variety of languages and
 frameworks including C, C++,Java etc.
* Remote Procedure Call is a protocol which one program can use to request a service from another program located in another computer in a network, without understanding
  network details.
* The client requires a way to know the exposed functions by this service and their parameters.And hence APAche thrift is used since it has its own
 “Interface Definition Language” (IDL).
* In IDL we can define the functions and their parameters.
• And we can use Thrift compiler to generate corresponding code for any language of your choice which means you can implement a function in Java,
  host it on a server and then remotely call it from Python or any other language for that matter.

2. Explain what is REST.

*REST can be expanded as Representational State Transfer.
*REST is based on existing web based technologies.
* The actual transport is using HTTP—which is the standard protocol for web applications.
* Hence REST is ideal for communicating between heterogeneous systems. 
* Internally, all the servers  use the common HTable-based client API to access the tables.
* They are started on top of the region server processes, which shares the same physical machine.
There is no one true recommendation for how to place the gateway servers.
* We can want to collocate them, or have them on dedicated machines

3. What is bulk import in Hbase?

Some of the bulk import techniques in Hbase are:

TableMapReduceUtil
*It is the  utility for TableMapper and TableReducer
addDependencyJars(org.apache.hadoop.mapreduce.Job job)
* Add the HBase dependency jars as well as jars for any of the configured job classes to the job configuration, so that JobClient will ship them to the cluster and add them to the DistributedCache.

initTableReducerJob(String table, Class<? extends TableReducer> reducer,
org.apache.hadoop.mapreduce.Job job)
* Use this before submitting a TableReduce job.
* sets TableOutputFormat as the output format

TableOutputFormat
* Convert Map/Reduce output and write it to an HBase table.
* Output value must be either a Put or a Delete instance

HFileOutputFormat
* Writes HFiles. Passed KeyValues must arrive in order. Currently, can only write files to a single column family at a time.

PutSortReducer
* Emits sorted Puts. Reads in all Puts from passed Iterator, sorts them, then emits Puts in sorted order.
* If lots of columns per row, it will use lots of memory sorting.

4. What is meant by a work flow and what is an oozie workflow.

A workflow is made up of an orchestrated and repeatable pattern of business activity enabled by the systematic organization of resources into processes which transforms materials, provides services, or processes information. 
It can be depicted as a sequence of operations, declared as work of a person or group, or one or more simple or complex mechanisms.

* Apache Oozie is a real time scheduler and workflow engine which blends well with large production environments
* It is a server based workflow engine
* Oozie can run workflow jobs with MapReduce and Pig action nodes
* Oozie Workflow jobs are Directed Acyclical Graphs (DAGs) of actions.• To run oozie workflows, two files are needed.
1. workflow.xml (stored in HDFS) which contains the structure of workflow.
2. job.properties (stored in local) which contains the configuration properties

5. How can you track a Oozie job.

Oozie job can be tracked using job tracker in job.properties

6. What kind of jobs can be scheduled with Oozie.


There are 2 basic types of Oozie jobs.They are
1)Oozie Workflow jobs are Directed Acyclical Graphs (DAGs) which specifies a sequence of actions to execute. The Workflow job has to wait.
2)Oozie Coordinator jobs are recurrent Oozie Workflow jobs which are triggered by time and data availability.

7. What is meant by oozie co-ordinator and how it is useful

*The Oozie Coordinator system allows the user to define and execute recurrent and interdependent workflow jobs.
*Coordinator application definitions can be parameterized with variables, built-in constants and built-in functions.

Coordinator Action: A coordinator action is a workflow job that is started when a set of conditions are met (input dataset instances are available).
Coordinator Application: A coordinator application defines the conditions under which coordinator actions should be created (the frequency) and when the actions can be started. The coordinator application also defines a start and an end time. Normally, coordinator applications are parameterized. A Coordinator application is written in XML.
Coordinator Job: A coordinator job is an executable instance of a coordination definition. A job submission is done by submitting a job configuration that resolves all parameters in the application definition.
Coordinator Definition Language: The language used to describe datasets and coordinator applications.
Coordinator Engine: A system that executes coordinator jobs.
Data pipeline: A data pipeline is a connected set of coordinator applications that consume and produce interdependent datasets.
